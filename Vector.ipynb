{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\40750\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"verified_online1.csv\") #fake\n",
    "df2=pd.read_csv(\"top-1m 2.csv\",header=None,names=[0,1]) #not-fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "urls = df1['url']\n",
    "urlst = urls.tolist()\n",
    "base_url_lst_fake=[]\n",
    "for i in range(urls.size):\n",
    "    url = urls[i]\n",
    "    base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))\n",
    "    if \"https://www.\" in base_url:\n",
    "        base_url=base_url.replace(\"https://www.\",\"\")\n",
    "    elif \"http://www\" in base_url:\n",
    "        base_url=base_url.replace(\"http://www.\",\"\")\n",
    "    elif \"http://\" in base_url:\n",
    "        base_url=base_url.replace(\"http://\",\"\")\n",
    "    elif \"https://\" in base_url:\n",
    "        base_url=base_url.replace(\"https://\",\"\")\n",
    "    base_url_lst_fake.append(base_url)\n",
    "for i in range(len(base_url_lst_fake)):\n",
    "    base_url_lst_fake[i]=base_url_lst_fake[i].replace(\"/\",\"\")\n",
    "\n",
    "\n",
    "base_url_lst_true=list(df2[1])\n",
    "base_url_lst_true=random.sample(base_url_lst_true,60000)\n",
    "#print(len(base_url_lst_true))\n",
    "y_true=[0 for _ in base_url_lst_true]\n",
    "y_false=[1 for _ in base_url_lst_fake]\n",
    "X_train_tosplit=base_url_lst_true+base_url_lst_fake\n",
    "y_train_tosplit=y_true+y_false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_train_tosplit,y_train_tosplit,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11717   319]\n",
      " [ 1761  4999]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.97      0.92     12036\n",
      "          1       0.94      0.74      0.83      6760\n",
      "\n",
      "avg / total       0.89      0.89      0.89     18796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred_class))\n",
    "print(classification_report(y_test,y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one vs rest accuracy: 0.8936475845924665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "classifier_ovr1 = Pipeline([('vectorizer', CountVectorizer()),('tfidf', TfidfTransformer()), ('clf', OneVsRestClassifier(SVC(kernel='linear')))])\n",
    "classifier_ovr1.fit(X_train, y_train)\n",
    "pred_y_ovr1 = classifier_ovr1.predict(X_test)\n",
    "results_ovr1 = [1 if val == y_test[inx] else 0 for inx, val in enumerate(pred_y_ovr1)]\n",
    "print('one vs rest accuracy:',float(sum(results_ovr1))/len(results_ovr1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'http://www.cadastro-seguro.ml',\n",
    "    'http://cadastro-seguro.ml',\n",
    "    'https://appleid-support-cloud.com/',\n",
    "    'http://appleid-support-cloud.com',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(df.URL)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_.get('importantupdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlsplit\n",
    "urls = df['URL']\n",
    "urlst = urls.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(urls.size):\n",
    "    url = urls[i]\n",
    "    base_url = \"{0.scheme}://{0.netloc}/\".format(urlsplit(url))\n",
    "    print(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
